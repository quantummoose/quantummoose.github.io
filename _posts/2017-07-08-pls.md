---
layout: post
title: "Partial Least Squares Regression"
date: 2017-07-08
categories: research
tagline: Combining PCA and regression
---

I've been using [partial least squares](https://en.wikipedia.org/wiki/Partial_least_squares_regression) analysis for a while now, but still learning about it. The core idea of PLS is really not complicated at all, but using it appropriately and fully understanding the model takes work.

What's the core idea? In short, PLS combines both principle components analysis and standard regression - instead of simply regressing Y against X, you find the best lower dimensions {% m %} Y_{low} {% em %} and {% m %} X_{low} {% em %} such that {% m %} Y_{low} {% em %} regresses against {% m %} X_{low} {% em %}. The code for running PLS in MATLAB is very simple, since there's a built in [plsregress](https://www.mathworks.com/help/stats/plsregress.html) function, but it's a bit obtuse{% sidenote "one" "Though this might be due to my own background" %}. I'm not going to go through [PCA](http://setosa.io/ev/principal-component-analysis/) and [regression](http://setosa.io/ev/ordinary-least-squares-regression/); I'm just going to assume we know those things.

For regression in general, we fit the equation {% m %} Y = \beta   X + \epsilon {% em %} where we want to predict Y from X, using the {% m %} \beta {% em %} regression weights or coefficients. {% m %} \epsilon {% em %} is just the error term here. So let's assume we want to predict some behavioral response, like reaction time, from some neural signal{% sidenote "two" "Assume all the values are centered." %}, so reaction time = Y {% sidenote "three" "Y is a *trial* x 1 vector" %} and brain signals = X {% sidenote "four" "X is a *trial* by *features* matrix, where features are just number of signals we have on each trial, like average voxel signal" %}. However, we measure a large amount of different features that may or may not be important. Presumably only some of them are.

First, let's generate some fake data in MATLAB:

``` matlab
% Generating fake data:
ntrials = 300;
% Only the hidden state determines response:
HiddenState = randn(1, ntrials);
Response = HiddenState * 5 + randn(1,ntrials);
plot(HiddenState, Response, '.')

% Now unrelated noise states:
OtherStates = randn(9, ntrials);
% Project those states into a higher dimension.
linproj = randn(10, 30);
XObs = [HiddenState', OtherStates'] * linproj;
```
{% marginfigure "one" "assets/img/pls/pls1.png" "plot(HiddenState, Response, '.')" %}

So what did I do here? I created some fake regression data, where our response is predicted from the Hidden State. However, I combined the hidden state with other random states that are unimportant for prediction. ``` linproj ``` is a matrix that basically puts your 10 states (combined hidden and noise) into a higher level space (30 dimensions). This kinda replicates what often happens to our measurement. We measure a bunch of signals that are highly correlated, with only a few actually important changing states. Now what happens if we go ahead and perform regression, regressing the 30 features of Xobs against the 1 Response?
``` matlab
>>> beta = XObs\Response'; % Solve simple regression in matlab.
Warning: Rank deficient, rank = 10, tol =  1.775230e-11.
```
So MATLAB already figured out that most of the dimensions aren't predictive. We accidentally got back we only have 10 states. Our regression here actually works pretty well (in terms of fit), but it's really overfitting all the noisy dimensions.

What if most of the features in X are either redundant or not really important for predicting? Well we could go ahead and perform PCA on the XObs to find what dimensions actually are important, {% m %} XObs = Xlow   U_L {% em %} where we only choose L components from the projection matrix U {% sidenote "five" "PCA decomposes X into two matrices, UDU' where U is a projection matrix and D is a diagonal matrix. SVD breaks a matrix into three matrices where U' = V." %}. In MATLAB we can do this by performing SVD on the covariance matrix of the features.
``` matlab
CObs = cov(XObs);
[U,S,V] = svd(CObs);
% Find percent variance explained:
cumsum(diag(S)/sum(diag(S)))
```
We find that we can explain over 80% of the variance with one component, and only need 10 components to explain everything. So we could go ahead and use those new lower dimensions for regression:
``` matlab
Xpca_scores = XObs * U(:,1:10);
beta2 = Xpca_scores\Response';
```
Again, if we only cared about fit here we do a good job, but again there's a worry of overfit. There's also another worry: if this was bad at predicting how do we assign blame? Which was the problem, the PCA or the regression? Now what if we wanted to *use* those hidden states; did we grab the *right* hidden states? If we wanted to somehow use the regression *with* the PCA to find the hidden state that's important at predicting the response, then we'd have to combine these ideas.

That's what PLS does. Partial Least Squares is named based on a comparison with the least squares method for regression{% sidenote "six", "Interestingly, the co-creater of PLS, Svante Wold, <a href=\"http://www.sciencedirect.com/science/article/pii/S0169743901001551\">prefers the name</a> *projection to latent structures*, which is much more explanatory." %}. PCA only minimizes reconstruction error, while regression minimizes least squares fit error. PLS tries to find the lower dimension that best predicts the response Y (using 'partial' least squares). It's a type of *directed dimensionality reduction*.

A way to visualize the model of PLS is below. This graphic shows each matrix that PLS is working with, and how they relate. Here our data of predictors is X with responses Y, and the scores refer to the points in a lower dimension space.
{% maincolumn "assets/img/pls/pls4.jpg" "PLS image taken from <a href=\"https://www.intechopen.com/books/multivariate-analysis-in-management-engineering-and-the-sciences/application-of-multivariate-data-analyses-in-waste-management\" ">here</a>. %}
To relate to our equations, our {% m %} Xscores = T {% em %} and {% m %} Yscores = U {% em %}, and the regression takes place as {% m %} U = B   T {% em %}, where {% m %} B {% em %} are the regression coefficients that relate X to Y through the lower dimensions.

In MATLAB, it's simple:
``` matlab
ncomp = 1; % Choose the number of components.
[Xlow, Ylow, Xscores, Yscores, beta] = plsregress(Xobs,Response',ncomp);
```
{% marginfigure "two" "assets/img/pls/pls2.png" "plot(Xscores, HiddenState, '.'). Note we don't completely recover the original states" %}
Note that PLS can deal with multiple Y dimensions as well, fitting a lower Y state (here Ylow, which we ignore). The scores are the projected lower dimension values, while the Xlow and Ylow are the projection matrices, so ``` Response = Yscores * Ylow' ``` and ``` XObs = Xscores *  Xlow' ```. The beta weights here are the weights{% sidenote "seven" "including intercept" %} *in the higher dimension space*, which is incredibly important. So ```Response = Xobs*beta(2:end)```, rather than being the weights in the lower dimension. You would have to project down using Xlow to get those values. In other words, {% m %} B = \beta \times  Xlow {% em %}.

Notice we had to choose a number of components. ```plsregress``` will actually output percent variance explained and mean squared error for fits from 1 to ncomp, so we can use that to decide the number of components. Which value we use largely depends on how we use the results of the model fit. Here we care mostly if we get the score values corresponding to the hidden state, so we have an external way to check.

For a geometric visualization of PLS, below we can see both the plane defined by the dimensionality reduction, and the line for the regression.
{% maincolumn "assets/img/pls/pls3.png" "From <a href=\"http://www.iasbs.ac.ir/chemistry/chemometrics/history/4th/5.pdf\">Wold, Sjostrom, and Eriksson 2001</a>" %}
The data from X fall on the plane, but follow the line (with noise). The line and components are chosen to induce a best correlation fit to the response Y.

That's a brief introduction into PLS, hopefully useful. I have a [gist of the code used](https://gist.github.com/quantummoose/ca12fea5c1285bbc9742d47bbc4dfc9b) if you'd like to see that. More detailed explanations exist for the model specifications if you're interested{% sidenote "eight" "Look at Kevin Murphy's *Machine Learning: a Probabilistic Approach* for a good comparison of the generative model of PLS to other methods, including CCA and PCA" %}. It's good for me to think through it sometimes.

PLS isn't the only method of performing directed dimensionality reduction, and it's also not the only way to deal with regression with high correlation in the features. For example, [canonical correlation](https://stats.stackexchange.com/questions/206587/what-is-the-connection-between-partial-least-squares-reduced-rank-regression-a) analysis is similar to PLS, but considers X and Y equivalent in that there isn't one 'predictor'. PLS is also used for [structural equation modeling](http://hbanaszak.mjr.uw.edu.pl/TempTxt/HaenleinKaplan_2004_BeginnersGuideToPLSAnalysis.pdf), a topic I understand very little, but is related in behavioral research.

Really it depends on the type of data you have, and the type of question you're interested in. The goal either way is to move beyond just simple regression, to get at the latent structure underlying the data.

Brief note: these are deeply related to [autoencoders](https://en.wikipedia.org/wiki/Autoencoder), a type of neural network. PLS basically specifies a particular network structures, based on the type of generative model.
